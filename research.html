<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Projects - Valeria Bodishtianu</title>
    <link rel="stylesheet" href="styles.css">
    <style>
        /* Collapsible style */
        .collapsible {
            cursor: pointer;
            padding: 10px;
            width: 100%;
            border: none;
            text-align: left;
            outline: none;
            font-size: 16px;
        }
        
        .active, .collapsible:hover {
            background-color: #f4f4f4;
        }
        
        .content {
            padding: 0 18px;
            display: none;
            overflow: hidden;
            background-color: #f9f9f9;
        }
    </style>
</head>
<body>
    <header>
        <nav>
            <a href="index.html">Home</a>
            <a href="projects.html">Projects</a>
            <a href="teaching.html">Teaching</a>
            <a href="contact.html">Contact</a>
        </nav>
    </header>
    
    <section id="projects">
        <h2>Working Papers</h2>
        
        <p><a>Polarization under Biased Argument Sharing</a> </p>
        <button type="button" class="collapsible">▼ Abstract</button>
        <div class="content">
            <p><strong>Abstract:</strong> This paper explores how self-censorship and biased argument sharing drive ideological polarization, supplanting homophily as the primary or necessary factor. I develop a formal model to explain why online polarization appears more extreme than offline and why interventions targeting echo chambers have largely failed. Contrary to conventional beliefs, I argue that anonymity is not the main cause of extreme behavior online; instead, both in anonymous and public settings, individuals self-censor to maintain a consistent ideological image, contributing to polarized discourse. Simulations using LLM-based agents show that self-censored argument sharing is consistently present in LLM agents, with relatively low levels of homophily, supporting the assumptions of our model. Results are particularly robust on large networks, making the model most suitable to exploring polarization in online settings.</p>
        </div>

        <h3><a>Information Aggregation in Presence of Media on a Network with Experts</a> </h3>
        <button type="button" class="collapsible">▼ Abstract</button>
        <div class="content">
            <p><strong>Abstract:</strong> The digital age has transformed the process of information spread and aggregation, giving
rise to social media platforms that challenge traditional news outlets' dominance in shaping
public opinion. This shift is accompanied by individual agents becoming content producers
in addition to consuming, thereby creating a more direct form of communication between
various parts of the social network. Historical reliance on traditional media has shifted
towards a more fragmented information landscape, where social media plays a significant
role in the dissemination and consumption of news. This transition raises important
questions about the nature of information aggregation in social networks and its capacity to
counteract biased narratives presented by mainstream media outlets.
In this paper I present a model of a communication system on a network between
inherently truth-seeking individuals. At the heart of my model is the role of knowledgeable agents (experts or witnesses), who serve as anchors of true information
within the network, countering the influence of slanted media signals. The central research
questions we address include how the distribution of knowledgeable agents impacts
information diffusion and the effectiveness of optimal placement strategies compared to
other policies aimed at reducing media bias.</p>
        </div>

        <h3><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4747531" target="_blank">Motivated Reasoning is Key to Fact-checking Behavior, and Money is Not</a>, with Dongfang Gaozhao and Pengfei Zhang. <em>Under review.</em> </h3>
        <button type="button" class="collapsible">▼ Abstract</button>
        <div class="content">
            <p><strong>Abstract:</strong> This paper investigates the cause and consequence of fact-checking. In an online experiment, we asked subjects to evaluate news veracity and varied two experimental conditions: (1) the opportunity to receive fact-checking results and (2) bonus payment for accuracy. We test three competing theories for fact-checking behavior: value of information (VoI), limited attention (LA), and motivated reasoning (MR). We find that monetary incentives do not promote fact-checking. Prior awareness of the news and perceived easiness in determining news authenticity significantly reduce fact-checking. Democrats are more likely to fact-check on the news aligning with Republicans' ideology, suggesting a tendency to seek information when there is a need to defend one's pre-existing belief. Overall, our results contradict VoI, show mixed evidence for LA, and support MR. When available, fact-checking consistently improves subjects' accuracy in evaluating news veracity by over 40\%.</p>
        </div>

        <h3><a href="https://arxiv.org/abs/2406.03299" target="_blank">The Good, the Bad, and the Hulk-like GPT: Analyzing Emotional Decisions of Large Language Models in Cooperation and Bargaining Games</a> (with Mikhail Mozikov, Nikita Severin, Maria Glushanina, Mikhail Baklashkin, Andrey V. Savchenko, Ilya Makarov)</h3>
        <button type="button" class="collapsible">▼ Abstract</button>
        <div class="content">
            <p><strong>Abstract:</strong> Behavior study experiments are an important part of society modeling and understanding human interactions. In practice, many behavioral experiments encounter challenges related to internal and external validity, reproducibility, and social bias due to the complexity of social interactions and cooperation in human user studies. Recent advances in Large Language Models (LLMs) have provided researchers with a new promising tool for the simulation of human behavior. However, existing LLM-based simulations operate under the unproven hypothesis that LLM agents behave similarly to humans as well as ignore a crucial factor in human decision-making: emotions.
In this paper, we introduce a novel methodology and the framework to study both, the decision-making of LLMs and their alignment with human behavior under emotional states. Experiments with GPT-3.5 and GPT-4 on four games from two different classes of behavioral game theory showed that emotions profoundly impact the performance of LLMs, leading to the development of more optimal strategies. While there is a strong alignment between the behavioral responses of GPT-3.5 and human participants, particularly evident in bargaining games, GPT-4 exhibits consistent behavior, ignoring induced emotions for rationality decisions. Surprisingly, emotional prompting, particularly with `anger' emotion, can disrupt the "superhuman" alignment of GPT-4, resembling human emotional responses.</p>
        </div>
        
        <h2>Publications</h2>
        
        <h3><a href="https://neurips.cc/" target="_blank">EAI: Emotional Decision-Making of LLMs in Strategic Games and Ethical Dilemmas</a> (with Mikhail Mozikov, Nikita Severin, Maria Glushanina, Mikhail Baklashkin, Ivan Nasonov, Daniil Orekhov, Ivan Makovetskiy, Vasily Lavrentyev, Vladislav Pekhotin, Akim Tsvigun, Denis Turdakov, Tatiana Shavrina, Andrey V. Savchenko, Ilya Makarov), in NeurIPS 2024</h3>
        <button type="button" class="collapsible">▼ Abstract</button>
        <div class="content">
            <p><strong>Abstract:</strong> One of the urgent tasks of artificial intelligence is to assess the safety and alignment of large language models (LLMs) with human behavior. Conventional verification in natural language processing problems only can be insufficient. Since human decisions are typically influenced by emotions, this paper studies the LLMs' alignment in complex strategic and ethical environments with an in-depth analysis of the drawbacks of our psychology and emotional impact on decision-making.

We introduce the novel EAI framework for integrating emotion modeling into LLMs to examine the emotional impact on ethics and LLM-based decision-making in a wide range of strategic games, including bargaining and repeated games. Our experimental study with various LLMs demonstrated that emotions can significantly alter the ethical decision-making landscape of LLMs, highlighting the need for robust mechanisms to ensure consistent ethical standards. The game-theoretic assessment showed that proprietary LLMs are prone to emotion biases that increase with decreasing model size or working with non-English languages. Moreover, adding emotions lets the LLMs increase the cooperation rate during the game.</p>
        </div>

        <h3><a href="https://aaai.org/papers/632-flairs-2017-15469/" target="_blank">Logic of Existentialism in Fiction</a> (with Ilya Makarov, in FLAIRS 2017)</h3>
        <button type="button" class="collapsible">▼ Abstract</button>
        <div class="content">
            <p><strong>Abstract:</strong> We have considered core approaches to the problem of fictional objects. For each model authors covered the problem
whether everything fictional exists or not in terms of evaluation, separating groups of objects, quantifying or existing in
modal worlds. The article contains brief overview of the approaches for dealing with fictional objects and evaluating
statements containing fictional objects as their part. </p>
        </div>

        <h3><a href="https://aaai.org/papers/412-flairs-2017-15463/" target="_blank">Adapting First-Person Shooter Video Game for Playing with Virtual Reality Headsets</a> (with Ilya Makarov, Oleg Konoplia, Pavel Polyakov, Maxim Martynov, Peter Zyuzin, Olga Gerasimova, in FLAIRS 2017)</h3>
        <button type="button" class="collapsible">▼ Abstract</button>
        <div class="content">
            <p><strong>Abstract:</strong> In this article a combination of two modern aspects of
games development is considered: (i) the impact of high
quality graphics and virtual reality (VR) user adaptation to
believe in realness of in-game events by user’s own eyes;
(ii) modeling an enemy’s behavior under automatic computer control, called BOT, which reacts similarly to human
players. We consider a First-Person Shooter (FPS) game
genre, which simulates an experience of combat actions. We
describe some tricks to overcome simulator sicknesses in a
shooter with respect to Oculus Rift and HTC Vive headsets.
We created a BOT model that strongly reduces the conflict
and uncertainty in matching human expectations. BOT
passes VR game Alan Turing test with 80% threshold of believable human-like behavior. </p>
        </div>
    </section>
    
    <footer>
        <p>&copy; 2024 Valeria Bodishtianu</p>
    </footer>

    <script>
        var coll = document.getElementsByClassName("collapsible");
        var i;

        for (i = 0; i < coll.length; i++) {
            coll[i].addEventListener("click", function() {
                this.classList.toggle("active");
                var content = this.nextElementSibling;
                if (content.style.display === "block") {
                    content.style.display = "none";
                } else {
                    content.style.display = "block";
                }
            });
        }
    </script>
</body>
</html>
