<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Projects - Valeria Bodishtianu</title>
    <link rel="stylesheet" href="styles.css">
    <style>
        /* Collapsible style */
        .collapsible {
            cursor: pointer;
            padding: 10px;
            width: 100%;
            border: none;
            text-align: left;
            outline: none;
            font-size: 16px;
        }
        
        .active, .collapsible:hover {
            background-color: #f4f4f4;
        }
        
        .content {
            padding: 0 18px;
            display: none;
            overflow: hidden;
            background-color: #f9f9f9;
        }
    </style>
</head>
<body>
    <header>
        <nav>
            <a href="index.html">Home</a>
            <a href="projects.html">Research</a>
            <a href="teaching.html">Teaching</a>
            <a href="contact.html">Contact</a>
        </nav>
    </header>
    
    <section id="projects">
        <h2>Working Papers</h2>
        
        <h3><a href = "https://www.dropbox.com/scl/fi/bhds4fy46u723hbl17e5m/Polarization_under_biased_argument_sharing__new.pdf?rlkey=7gc0nrs8lbu9nrwhj3eb1ig58&st=q7v6cg7u&dl=0" target="_blank"><strong>Polarization under Biased Argument Sharing</strong></a>. </h3>
        <button type="button" class="collapsible">▼ Abstract</button>
        <div class="content">
            <p><strong>Abstract:</strong> This paper explores how self-censorship and biased argument sharing drive ideological dynamics, challenging homophily as the primary factor. 
                I develop a formal model to explain why online polarization appears more extreme than offline and why interventions targeting echo chambers have largely failed. I argue that the prevalence of small, extreme groups online is a direct consequence of self-censoring behavior 
                in order to maintain a consistent ideological image on networks with sufficiently high density. I further show that the relationship between network effects and homophily is non-monotonic, which explains the failure of policy and platform interventions aimed at modifying network structure to reduce polarization. I use LLMs as a unique tool for testing these patterns, owing to their strong alignment with human behavioral dynamics. By embedding LLM agents in a network framework, I examine how self-censored argument sharing emerges under various conditions, showing it persists even with low homophily, supporting the model’s assumptions. I further validate the results using data from real social networks, making this approach especially relevant for exploring ideological dynamics in digital spaces.</p>
        </div>
        
        <h3><a href="https://www.dropbox.com/scl/fi/zjtkbx3b3uc4rkxeruqrl/Experts_paper.pdf?rlkey=us6auoq7bkt6nb1ipmg7tm3dp&st=t3t1hhl4&dl=0"target="_blank" rel="noopener"><strong>Media Distortion and Expert Correction on Social Networks</strong></a></h3>

        <button type="button" class="collapsible">▼ Abstract</button>
        <div class="content">
            <p><strong>Abstract:</strong> In this paper, I explore information aggregation in social networks amidst growing competition between traditional media and social platforms. I model a network of truth-seeking agents who receive information from both their neighbors and a potentially biased media source. Knowledgeable agents, or “experts,” act as anchors for accurate information within this network. Key questions include how the network’s structure and the placement of experts influence the effectiveness of countering biased narratives. My results reveal that an agent’s influence is tied to their Katz-Bonacich centrality, with simulations indicating that merely increasing the visibility of knowledgeable agents, without reinforcing their credibility, may backfire, leading to skepticism. This study underscores that, to mitigate bias, policy and platform efforts should prioritize building the credibility and reputation of accurate sources - through measures like external validation mechanisms - over extending their reach.</p>
        </div>

        <h3><strong><a href="https://arxiv.org/abs/2406.03299" target="_blank">The Good, the Bad, and the Hulk-like GPT: Analyzing Emotional Decisions of Large Language Models in Cooperation and Bargaining Games</a></strong>, with Mikhail Mozikov, Nikita Severin, Maria Glushanina, Mikhail Baklashkin, Andrey V. Savchenko, Ilya Makarov.</h3>
        <button type="button" class="collapsible">▼ Abstract</button>
        <div class="content">
            <p><strong>Abstract:</strong> Behavior study experiments are an important part of society modeling and understanding human interactions. In practice, many behavioral experiments encounter challenges related to internal and external validity, reproducibility, and social bias due to the complexity of social interactions and cooperation in human user studies. Recent advances in Large Language Models (LLMs) have provided researchers with a new promising tool for the simulation of human behavior. However, existing LLM-based simulations operate under the unproven hypothesis that LLM agents behave similarly to humans as well as ignore a crucial factor in human decision-making: emotions.
In this paper, we introduce a novel methodology and the framework to study both, the decision-making of LLMs and their alignment with human behavior under emotional states. Experiments with GPT-3.5 and GPT-4 on four games from two different classes of behavioral game theory showed that emotions profoundly impact the performance of LLMs, leading to the development of more optimal strategies. While there is a strong alignment between the behavioral responses of GPT-3.5 and human participants, particularly evident in bargaining games, GPT-4 exhibits consistent behavior, ignoring induced emotions for rationality decisions. Surprisingly, emotional prompting, particularly with `anger' emotion, can disrupt the "superhuman" alignment of GPT-4, resembling human emotional responses.</p>
        </div>
        
        <h3><strong>Sovereign Rating Changes and FDI to Emerging Markets: Fear over Greed</strong>, with Kaushik Basu and Supriyo De.</h3>
        
        <button type="button" class="collapsible">▼ Abstract</button>
        <div class="content">
            <p><strong>Abstract:</strong> This paper explores how Sovereign Credit Rating changes influence Foreign Direct Investment (FDI) inflows in emerging and developing economies during the post-crisis period. In light of diminished trust in credit ratings after the 2008 financial crisis, markets have become more skeptical of these ratings as indicators of economic stability. By examining both absolute and relative rating changes, this study aims to understand how the market response to credit rating depends on timing and direction of change. The results show that upward rating changes, while not affecting FDI inflows immediately, have a positive impact in the following period, suggesting market hesitancy - investors may wait to see if the improvement holds before committing. On the other hand, downward rating changes cause an immediate decline in FDI, with no significant lagged effect, suggesting sharp but short-term market reaction. Relative Rating (RR) shifts on average have nearly double the impact of absolute shifts, underscoring the importance of comparative ratings over standalone assessments for investors. </p>
        </div>

        <h2>Publications</h2>
        
        <h3><strong><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0323105" target="_blank">The Motivation and Consequence of Fact-checking Behavior: An Experimental Study</a></strong>, with Dongfang Gaozhao and Pengfei Zhang. <em>In: PLoS One (2025), 20(5): e0323105.</em> [<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4747531" target="_blank" rel="noopener">Working paper (SSRN)</a>]</h3>
        <button type="button" class="collapsible">▼ Abstract</button>
        <div class="content">
            <p><strong>Abstract:</strong> In a series of online experiments, we asked people to evaluate news veracity and varied two experimental conditions: (1) the opportunity to receive fact-checking results and (2) bonus payment for accuracy. We tested three competing theories for fact-checking behavior: value of information (VoI), limited attention (LA), and motivated reasoning (MR). We find that monetary incentives do not promote fact-checking. Prior awareness of the news and perceived easiness in determining news authenticity significantly reduce fact-checking. Democrats are more likely to fact-check on the news aligning with Republicans’ ideology, suggesting a tendency to seek information when there is a need to defend one’s pre-existing belief. Overall, our results contradict VoI, show mixed evidence for MR, and support LA. When available, fact-checking consistently improves subjects’ accuracy in evaluating news veracity by over 40%, underscoring the importance of promoting fact-checking in curbing misinformation.</p>
        </div>
        <h3><strong><a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/611e84703eac7cc03f78339df8aae2ed-Abstract-Conference.html" target="_blank">EAI: Emotional Decision-Making of LLMs in Strategic Games and Ethical Dilemmas</a></strong>, with Mikhail Mozikov, Nikita Severin, Maria Glushanina, Mikhail Baklashkin, Ivan Nasonov, Daniil Orekhov, Ivan Makovetskiy, Vasily Lavrentyev, Vladislav Pekhotin, Akim Tsvigun, Denis Turdakov, Tatiana Shavrina, Andrey V. Savchenko, Ilya Makarov. <em>In: Advances in Neural Information Processing Systems 37 (NeurIPS 2024).</em></h3>
        <button type="button" class="collapsible">▼ Abstract</button>
        <div class="content">
            <p><strong>Abstract:</strong> One of the urgent tasks of artificial intelligence is to assess the safety and alignment of large language models (LLMs) with human behavior. Conventional verification in natural language processing problems only can be insufficient. Since human decisions are typically influenced by emotions, this paper studies the LLMs' alignment in complex strategic and ethical environments with an in-depth analysis of the drawbacks of our psychology and emotional impact on decision-making.

We introduce the novel EAI framework for integrating emotion modeling into LLMs to examine the emotional impact on ethics and LLM-based decision-making in a wide range of strategic games, including bargaining and repeated games. Our experimental study with various LLMs demonstrated that emotions can significantly alter the ethical decision-making landscape of LLMs, highlighting the need for robust mechanisms to ensure consistent ethical standards. The game-theoretic assessment showed that proprietary LLMs are prone to emotion biases that increase with decreasing model size or working with non-English languages. Moreover, adding emotions lets the LLMs increase the cooperation rate during the game.</p>
        </div>

        <h3><strong><a href="https://aaai.org/papers/632-flairs-2017-15469/" target="_blank">Logic of Existentialism in Fiction</a></strong>, with Ilya Makarov. <em>In: Proceedings of the Thirtieth International Florida Artificial Intelligence Research Society Conference (FLAIRS), 2017.</em></h3>
        <button type="button" class="collapsible">▼ Abstract</button>
        <div class="content">
            <p><strong>Abstract:</strong> We have considered core approaches to the problem of fictional objects. For each model authors covered the problem
whether everything fictional exists or not in terms of evaluation, separating groups of objects, quantifying or existing in
modal worlds. The article contains brief overview of the approaches for dealing with fictional objects and evaluating
statements containing fictional objects as their part. </p>
        </div>

        <h3><strong><a href="https://aaai.org/papers/412-flairs-2017-15463/" target="_blank">Adapting First-Person Shooter Video Game for Playing with Virtual Reality Headsets</a></strong>, with Ilya Makarov, Oleg Konoplia, Pavel Polyakov, Maxim Martynov, Peter Zyuzin, Olga Gerasimova. <em>In: Proceedings of the Thirtieth International Florida Artificial Intelligence Research Society Conference (FLAIRS), 2017.</em> </h3>
        <button type="button" class="collapsible">▼ Abstract</button>
        <div class="content">
            <p><strong>Abstract:</strong> In this article a combination of two modern aspects of
games development is considered: (i) the impact of high
quality graphics and virtual reality (VR) user adaptation to
believe in realness of in-game events by user’s own eyes;
(ii) modeling an enemy’s behavior under automatic computer control, called BOT, which reacts similarly to human
players. We consider a First-Person Shooter (FPS) game
genre, which simulates an experience of combat actions. We
describe some tricks to overcome simulator sicknesses in a
shooter with respect to Oculus Rift and HTC Vive headsets.
We created a BOT model that strongly reduces the conflict
and uncertainty in matching human expectations. BOT
passes VR game Alan Turing test with 80% threshold of believable human-like behavior. </p>
        </div>
    </section>
    
    <footer>
        <p>&copy; 2024 Valeria Bodishtianu</p>
    </footer>

    <script>
        var coll = document.getElementsByClassName("collapsible");
        var i;

        for (i = 0; i < coll.length; i++) {
            coll[i].addEventListener("click", function() {
                this.classList.toggle("active");
                var content = this.nextElementSibling;
                if (content.style.display === "block") {
                    content.style.display = "none";
                } else {
                    content.style.display = "block";
                }
            });
        }
    </script>
</body>
</html>
